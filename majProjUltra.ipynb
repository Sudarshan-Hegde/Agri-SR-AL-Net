{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adad93b8",
   "metadata": {},
   "source": [
    "# RFB-ESRGAN for BigEarthNet Super-Resolution\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "**IMPORTANT:** Run Cell 1 ONCE to install dependencies, then:\n",
    "1. **Restart the kernel** (Kernel → Restart in Kaggle)\n",
    "2. **Skip Cell 1** and run directly from Cell 2 onwards\n",
    "\n",
    "This is necessary to clear cached PyTorch/torchvision versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a76e283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.6.0+cu118\n",
      "Uninstalling torch-2.6.0+cu118:\n",
      "Uninstalling torch-2.6.0+cu118:\n",
      "  Successfully uninstalled torch-2.6.0+cu118\n",
      "  Successfully uninstalled torch-2.6.0+cu118\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "Found existing installation: torchvision 0.21.0+cu124\n",
      "Uninstalling torchvision-0.21.0+cu124:\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "  Successfully uninstalled torchvision-0.21.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Found existing installation: torchaudio 2.6.0+cu124\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "Uninstalling torchaudio-2.6.0+cu124:\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "  Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/780.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/14.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/664.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/410.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/121.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/124.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/196.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/99.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m✓ All dependencies installed successfully!\n",
      "⚠️ If you see CUDA version mismatch errors, restart the kernel and run from Cell 2\n",
      "✓ All dependencies installed successfully!\n",
      "⚠️ If you see CUDA version mismatch errors, restart the kernel and run from Cell 2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_48/4097762052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Don't re-order these, we need to load the _C extension (done when importing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# .extensions) before entering _meta_registrations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mextension\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_HAS_OPS\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_meta_registrations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mutils\u001b[0m  \u001b[0;31m# usort:skip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m \u001b[0m_check_cuda_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchvision/extension.py\u001b[0m in \u001b[0;36m_check_cuda_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mt_minor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_version\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt_major\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtv_major\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34m\"Detected that PyTorch and torchvision were compiled with different CUDA major versions. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"PyTorch has CUDA Version={t_major}.{t_minor} and torchvision has \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Detected that PyTorch and torchvision were compiled with different CUDA major versions. PyTorch has CUDA Version=11.8 and torchvision has CUDA Version=12.4. Please reinstall the torchvision that matches your PyTorch install."
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "\n",
    "# Install required dependencies\n",
    "# Uninstall old versions and install compatible PyTorch/torchvision\n",
    "!pip uninstall -y torch torchvision torchaudio\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q wandb tqdm pillow numpy opencv-python scikit-image\n",
    "\n",
    "print(\"✓ All dependencies installed successfully!\")\n",
    "print(\"⚠️ If you see CUDA version mismatch errors, restart the kernel and run from Cell 2\")\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "from collections import OrderedDict\n",
    "import time\n",
    "\n",
    "# WandB setup\n",
    "wandb.login(key='5424a3d65aac1662f5be82d4439aaac35046689e')\n",
    "wandb.init(\n",
    "    project='RFB-ESRGAN-BigEarthNet',\n",
    "    config={\n",
    "        'upscale_factor': 16,\n",
    "        'lr_size': 32,\n",
    "        'hr_size': 512,\n",
    "        'batch_size': 8,\n",
    "        'stage1_epochs': 50,\n",
    "        'stage2_iters': 100000,\n",
    "        'stage1_lr': 2e-4,\n",
    "        'stage2_lr': 1e-4,\n",
    "        'lambda_pix': 10,\n",
    "        'lambda_vgg': 1,\n",
    "        'lambda_adv': 5e-3,\n",
    "        'num_rrdb': 16,\n",
    "        'num_rrfdb': 8,\n",
    "        'ensemble_models': 10\n",
    "    }\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "print(f'PyTorch version: {torch.__version__}')\n",
    "print(f'Torchvision version: {torchvision.__version__}')\n",
    "print(f'CUDA available: {torch.cuda.is_available()}')\n",
    "if torch.cuda.is_available():\n",
    "    print(f'CUDA version: {torch.version.cuda}')\n",
    "    print(f'Number of GPUs available: {torch.cuda.device_count()}')\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfab5b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Model Architecture - RFB, RRDB, RRFDB Blocks\n",
    "\n",
    "class RFB(nn.Module):\n",
    "    \"\"\"Receptive Field Block - Multi-scale feature extraction with small kernels\"\"\"\n",
    "    def __init__(self, in_channels=64):\n",
    "        super(RFB, self).__init__()\n",
    "        # Branch 1: AvgPool(3) + 1x1 conv + dilated 3x3 (d=1)\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.AvgPool2d(3, stride=1, padding=1),\n",
    "            nn.Conv2d(in_channels, 16, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(16, 16, 3, 1, padding=1, dilation=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Branch 2: AvgPool(5) + 1x1 conv + dilated 3x3 (d=2)\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.AvgPool2d(5, stride=1, padding=2),\n",
    "            nn.Conv2d(in_channels, 24, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 24, 3, 1, padding=2, dilation=2),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Branch 3: AvgPool(7) + 1x1 conv + dilated 3x3 (d=3)\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.AvgPool2d(7, stride=1, padding=3),\n",
    "            nn.Conv2d(in_channels, 24, 1, 1, 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(24, 24, 3, 1, padding=3, dilation=3),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Concat 16+24+24=64 → 1x1 conv to 64\n",
    "        self.conv_concat = nn.Sequential(\n",
    "            nn.Conv2d(64, in_channels, 1, 1, 0),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        concat = torch.cat([b1, b2, b3], dim=1)\n",
    "        out = self.conv_concat(concat)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DenseBlock(nn.Module):\n",
    "    \"\"\"Dense Block with 5 convolutions (from ESRGAN RRDB)\"\"\"\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super(DenseBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(nf, gc, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(nf + gc, gc, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(nf + 2 * gc, gc, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(nf + 3 * gc, gc, 3, 1, 1)\n",
    "        self.conv5 = nn.Conv2d(nf + 4 * gc, nf, 3, 1, 1)\n",
    "        self.lrelu = nn.LeakyReLU(0.2, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.lrelu(self.conv1(x))\n",
    "        x2 = self.lrelu(self.conv2(torch.cat([x, x1], dim=1)))\n",
    "        x3 = self.lrelu(self.conv3(torch.cat([x, x1, x2], dim=1)))\n",
    "        x4 = self.lrelu(self.conv4(torch.cat([x, x1, x2, x3], dim=1)))\n",
    "        x5 = self.conv5(torch.cat([x, x1, x2, x3, x4], dim=1))\n",
    "        return x5 * 0.2 + x  # Residual scaling\n",
    "\n",
    "\n",
    "class RRDB(nn.Module):\n",
    "    \"\"\"Residual-in-Residual Dense Block (ESRGAN)\"\"\"\n",
    "    def __init__(self, nf=64, gc=32):\n",
    "        super(RRDB, self).__init__()\n",
    "        self.db1 = DenseBlock(nf, gc)\n",
    "        self.db2 = DenseBlock(nf, gc)\n",
    "        self.db3 = DenseBlock(nf, gc)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.db1(x)\n",
    "        out = self.db2(out)\n",
    "        out = self.db3(out)\n",
    "        return out * 0.2 + x  # Residual scaling\n",
    "\n",
    "\n",
    "class RRFDB(nn.Module):\n",
    "    \"\"\"Residual Receptive Field Dense Block (5 RFBs in dense style)\"\"\"\n",
    "    def __init__(self, nf=64):\n",
    "        super(RRFDB, self).__init__()\n",
    "        self.rfb1 = RFB(nf)\n",
    "        self.rfb2 = RFB(nf)\n",
    "        self.rfb3 = RFB(nf)\n",
    "        self.rfb4 = RFB(nf)\n",
    "        self.rfb5 = RFB(nf)\n",
    "        # Simple dense connection via addition (simplified from paper)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.rfb1(x)\n",
    "        out = self.rfb2(out)\n",
    "        out = self.rfb3(out)\n",
    "        out = self.rfb4(out)\n",
    "        out = self.rfb5(out)\n",
    "        return out * 0.2 + x  # Residual scaling\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"RFB-ESRGAN Generator (20.5M params, x16 upscale)\"\"\"\n",
    "    def __init__(self, num_rrdb=16, num_rrfdb=8, nf=64):\n",
    "        super(Generator, self).__init__()\n",
    "        # First conv\n",
    "        self.conv_first = nn.Conv2d(3, nf, 3, 1, 1)\n",
    "        \n",
    "        # Trunk-A: 16 RRDBs\n",
    "        self.trunk_a = nn.Sequential(*[RRDB(nf) for _ in range(num_rrdb)])\n",
    "        \n",
    "        # Trunk-RFB: 8 RRFDBs\n",
    "        self.trunk_rfb = nn.Sequential(*[RRFDB(nf) for _ in range(num_rrfdb)])\n",
    "        \n",
    "        # Single RFB before upsampling\n",
    "        self.rfb_up = RFB(nf)\n",
    "        \n",
    "        # Alternating upsampling: inter → sub → inter → sub (x16 total)\n",
    "        # x2 nearest → x2 pixelshuffle → x2 nearest → x2 pixelshuffle\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # x2\n",
    "            nn.Conv2d(nf, nf * 4, 3, 1, 1),\n",
    "            nn.PixelShuffle(2),  # x2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),  # x2\n",
    "            nn.Conv2d(nf, nf * 4, 3, 1, 1),\n",
    "            nn.PixelShuffle(2),  # x2\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Final convs\n",
    "        self.conv_final = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf, 3, 1, 1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(nf, 3, 3, 1, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.conv_first(x)\n",
    "        trunk_a_out = self.trunk_a(feat)\n",
    "        trunk_rfb_out = self.trunk_rfb(trunk_a_out)\n",
    "        rfb_out = self.rfb_up(trunk_rfb_out)\n",
    "        upsampled = self.upsample(rfb_out)\n",
    "        out = self.conv_final(upsampled)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"ESRGAN-style Discriminator with spectral norm\"\"\"\n",
    "    def __init__(self, in_channels=3, nf=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def conv_block(in_c, out_c, stride=1, norm=True):\n",
    "            layers = [nn.Conv2d(in_c, out_c, 3, stride, 1)]\n",
    "            if norm:\n",
    "                layers.append(nn.BatchNorm2d(out_c))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            conv_block(in_channels, nf, 1, False),\n",
    "            conv_block(nf, nf, 2),\n",
    "            conv_block(nf, nf * 2, 1),\n",
    "            conv_block(nf * 2, nf * 2, 2),\n",
    "            conv_block(nf * 2, nf * 4, 1),\n",
    "            conv_block(nf * 4, nf * 4, 2),\n",
    "            conv_block(nf * 4, nf * 8, 1),\n",
    "            conv_block(nf * 8, nf * 8, 2),\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(nf * 8, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        feat = self.features(x)\n",
    "        out = self.classifier(feat)\n",
    "        return out\n",
    "\n",
    "print(\"Architecture defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcceed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Loss Functions\n",
    "\n",
    "class VGGPerceptualLoss(nn.Module):\n",
    "    \"\"\"VGG19 conv3_4 perceptual loss (L_VGG)\"\"\"\n",
    "    def __init__(self):\n",
    "        super(VGGPerceptualLoss, self).__init__()\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).features\n",
    "        self.vgg_layers = nn.Sequential(*list(vgg.children())[:16])  # Up to conv3_4\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.vgg_layers.eval()\n",
    "        \n",
    "        # ImageNet normalization\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "        \n",
    "    def forward(self, sr, hr):\n",
    "        # Normalize from [-1,1] to ImageNet range\n",
    "        sr = (sr + 1) / 2  # [0,1]\n",
    "        hr = (hr + 1) / 2\n",
    "        sr = (sr - self.mean) / self.std\n",
    "        hr = (hr - self.mean) / self.std\n",
    "        \n",
    "        sr_feat = self.vgg_layers(sr)\n",
    "        hr_feat = self.vgg_layers(hr)\n",
    "        return F.l1_loss(sr_feat, hr_feat)\n",
    "\n",
    "\n",
    "class GANLoss(nn.Module):\n",
    "    \"\"\"Relativistic GAN loss from ESRGAN\"\"\"\n",
    "    def __init__(self):\n",
    "        super(GANLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, d_real, d_fake, is_disc=False):\n",
    "        if is_disc:\n",
    "            # Discriminator loss: L_D = -E[log(Delta_Real)] - E[1-log(Delta_Fake)]\n",
    "            delta_real = torch.sigmoid(d_real - d_fake.mean())\n",
    "            delta_fake = torch.sigmoid(d_fake - d_real.mean())\n",
    "            loss_real = -torch.log(delta_real + 1e-8).mean()\n",
    "            loss_fake = -torch.log(1 - delta_fake + 1e-8).mean()\n",
    "            return loss_real + loss_fake\n",
    "        else:\n",
    "            # Generator adversarial loss: L_adv = -E[log(1-Delta_Real)] - E[log(Delta_Fake)]\n",
    "            delta_real = torch.sigmoid(d_real - d_fake.mean())\n",
    "            delta_fake = torch.sigmoid(d_fake - d_real.mean())\n",
    "            loss = -torch.log(1 - delta_real + 1e-8).mean() - torch.log(delta_fake + 1e-8).mean()\n",
    "            return loss\n",
    "\n",
    "\n",
    "def compute_generator_loss(sr, hr, d_real, d_fake, vgg_loss_fn, gan_loss_fn, lambda_pix=10, lambda_vgg=1, lambda_adv=5e-3):\n",
    "    \"\"\"Total generator loss: L_G = λ*L_pix + L_VGG + η*L_adv\"\"\"\n",
    "    l_pix = F.l1_loss(sr, hr)\n",
    "    l_vgg = vgg_loss_fn(sr, hr)\n",
    "    l_adv = gan_loss_fn(d_real, d_fake, is_disc=False)\n",
    "    \n",
    "    total_loss = lambda_pix * l_pix + lambda_vgg * l_vgg + lambda_adv * l_adv\n",
    "    \n",
    "    return total_loss, l_pix, l_vgg, l_adv\n",
    "\n",
    "print(\"Loss functions defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce835eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Dataset and DataLoader for BigEarthNet\n",
    "\n",
    "class BigEarthNetDataset(Dataset):\n",
    "    \"\"\"BigEarthNet dataset for super-resolution\"\"\"\n",
    "    def __init__(self, data_dir, hr_size=512, lr_size=32, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.hr_size = hr_size\n",
    "        self.lr_size = lr_size\n",
    "        \n",
    "        # Find all image files (adjust pattern based on BigEarthNet structure)\n",
    "        self.image_paths = glob.glob(os.path.join(data_dir, '**/*.jpg'), recursive=True) + \\\n",
    "                          glob.glob(os.path.join(data_dir, '**/*.png'), recursive=True) + \\\n",
    "                          glob.glob(os.path.join(data_dir, '**/*.tif'), recursive=True)\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} images in {data_dir}\")\n",
    "        \n",
    "        # Augmentation - Random flips and 90-degree rotations\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.RandomChoice([\n",
    "                transforms.RandomRotation([0, 0]),\n",
    "                transforms.RandomRotation([90, 90]),\n",
    "                transforms.RandomRotation([180, 180]),\n",
    "                transforms.RandomRotation([270, 270]),\n",
    "            ])\n",
    "        ]) if transform is None else transform\n",
    "        \n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            # Random crop to hr_size\n",
    "            w, h = img.size\n",
    "            if w < self.hr_size or h < self.hr_size:\n",
    "                # Resize if image is too small\n",
    "                img = img.resize((max(w, self.hr_size), max(h, self.hr_size)), Image.BICUBIC)\n",
    "                w, h = img.size\n",
    "            \n",
    "            left = np.random.randint(0, w - self.hr_size + 1)\n",
    "            top = np.random.randint(0, h - self.hr_size + 1)\n",
    "            hr_img = img.crop((left, top, left + self.hr_size, top + self.hr_size))\n",
    "            \n",
    "            # Apply augmentation\n",
    "            hr_img = self.transform(hr_img)\n",
    "            \n",
    "            # Create LR via bicubic downsampling\n",
    "            lr_img = hr_img.resize((self.lr_size, self.lr_size), Image.BICUBIC)\n",
    "            \n",
    "            # Convert to tensor and normalize to [-1, 1]\n",
    "            hr_tensor = self.to_tensor(hr_img) * 2 - 1\n",
    "            lr_tensor = self.to_tensor(lr_img) * 2 - 1\n",
    "            \n",
    "            return lr_tensor, hr_tensor\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return random tensors on error\n",
    "            return torch.randn(3, self.lr_size, self.lr_size), torch.randn(3, self.hr_size, self.hr_size)\n",
    "\n",
    "\n",
    "# Setup datasets - BigEarthNet V2 S2 paths\n",
    "BIGEARTHNET_DIR = '/kaggle/input/bigearthnetv2-s2-4/BigEarthNet-S2'\n",
    "LABEL_DIR = '/kaggle/input/label-indices'\n",
    "\n",
    "# Train/Val/Test CSVs for reference\n",
    "TRAIN_CSV = os.path.join(LABEL_DIR, 'train.csv')\n",
    "VAL_CSV = os.path.join(LABEL_DIR, 'val.csv')\n",
    "TEST_CSV = os.path.join(LABEL_DIR, 'test.csv')\n",
    "\n",
    "# If running locally for testing, use a local path\n",
    "if not os.path.exists(BIGEARTHNET_DIR):\n",
    "    print(\"⚠️ BigEarthNet dataset not found. Please update paths for local testing.\")\n",
    "    BIGEARTHNET_DIR = './bigearthnet_sample'  # Fallback for local testing\n",
    "\n",
    "train_dataset = BigEarthNetDataset(BIGEARTHNET_DIR, hr_size=512, lr_size=32)\n",
    "val_dataset = BigEarthNetDataset(BIGEARTHNET_DIR, hr_size=512, lr_size=32)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=wandb.config.batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa525bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Loop (Stage 1: PSNR + Stage 2: GAN)\n",
    "\n",
    "def train_stage1(generator, train_loader, val_loader, epochs=50, lr=2e-4):\n",
    "    \"\"\"Stage 1: PSNR-oriented training with L1 loss\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STAGE 1: PSNR-ORIENTED TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.99))\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    \n",
    "    generator.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for lr_img, hr_img in pbar:\n",
    "            lr_img = lr_img.to(device)\n",
    "            hr_img = hr_img.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            sr_img = generator(lr_img)\n",
    "            loss = F.l1_loss(sr_img, hr_img)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pbar.set_postfix({'L1 Loss': f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation\n",
    "        generator.eval()\n",
    "        val_psnr = 0\n",
    "        with torch.no_grad():\n",
    "            for lr_img, hr_img in val_loader:\n",
    "                lr_img = lr_img.to(device)\n",
    "                hr_img = hr_img.to(device)\n",
    "                sr_img = generator(lr_img)\n",
    "                mse = F.mse_loss(sr_img, hr_img)\n",
    "                psnr = 10 * torch.log10(4 / mse)  # Range [-1,1] → max=2, so 4\n",
    "                val_psnr += psnr.item()\n",
    "        val_psnr /= len(val_loader)\n",
    "        generator.train()\n",
    "        \n",
    "        wandb.log({\n",
    "            'stage1/epoch': epoch + 1,\n",
    "            'stage1/train_loss': avg_loss,\n",
    "            'stage1/val_psnr': val_psnr,\n",
    "            'stage1/lr': optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}: Train Loss={avg_loss:.4f}, Val PSNR={val_psnr:.2f} dB\")\n",
    "    \n",
    "    # Save Stage 1 checkpoint - handle DataParallel wrapper\n",
    "    model_state = generator.module.state_dict() if isinstance(generator, nn.DataParallel) else generator.state_dict()\n",
    "    torch.save(model_state, 'generator_stage1.pth')\n",
    "    print(\"✓ Stage 1 complete. Model saved to generator_stage1.pth\")\n",
    "\n",
    "\n",
    "def train_stage2(generator, discriminator, train_loader, val_loader, iterations=100000, lr=1e-4):\n",
    "    \"\"\"Stage 2: GAN training with perceptual losses\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STAGE 2: GAN TRAINING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    optimizer_g = torch.optim.Adam(generator.parameters(), lr=lr, betas=(0.9, 0.99))\n",
    "    optimizer_d = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(0.9, 0.99))\n",
    "    \n",
    "    # LR decay schedule\n",
    "    milestones = [50000, 100000]\n",
    "    scheduler_g = torch.optim.lr_scheduler.MultiStepLR(optimizer_g, milestones=milestones, gamma=0.5)\n",
    "    scheduler_d = torch.optim.lr_scheduler.MultiStepLR(optimizer_d, milestones=milestones, gamma=0.5)\n",
    "    \n",
    "    vgg_loss_fn = VGGPerceptualLoss().to(device)\n",
    "    gan_loss_fn = GANLoss()\n",
    "    \n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    \n",
    "    saved_models = []  # Track top-10 models for ensemble\n",
    "    iter_count = 0\n",
    "    \n",
    "    while iter_count < iterations:\n",
    "        for lr_img, hr_img in train_loader:\n",
    "            if iter_count >= iterations:\n",
    "                break\n",
    "                \n",
    "            lr_img = lr_img.to(device)\n",
    "            hr_img = hr_img.to(device)\n",
    "            \n",
    "            # ========== Train Discriminator ==========\n",
    "            optimizer_d.zero_grad()\n",
    "            sr_img = generator(lr_img).detach()\n",
    "            d_real = discriminator(hr_img)\n",
    "            d_fake = discriminator(sr_img)\n",
    "            loss_d = gan_loss_fn(d_real, d_fake, is_disc=True)\n",
    "            loss_d.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # ========== Train Generator ==========\n",
    "            optimizer_g.zero_grad()\n",
    "            sr_img = generator(lr_img)\n",
    "            d_real = discriminator(hr_img).detach()\n",
    "            d_fake = discriminator(sr_img)\n",
    "            \n",
    "            # Total generator loss: L_G = L_pix + λ_vgg*L_vgg + λ_adv*L_adv\n",
    "            l_pix = F.l1_loss(sr_img, hr_img)\n",
    "            l_vgg = vgg_loss_fn(sr_img, hr_img)\n",
    "            l_adv = gan_loss_fn(d_real, d_fake, is_disc=False)\n",
    "            \n",
    "            loss_g = (\n",
    "                wandb.config.lambda_pix * l_pix +\n",
    "                wandb.config.lambda_vgg * l_vgg +\n",
    "                wandb.config.lambda_adv * l_adv\n",
    "            )\n",
    "            loss_g.backward()\n",
    "            optimizer_g.step()\n",
    "            \n",
    "            scheduler_g.step()\n",
    "            scheduler_d.step()\n",
    "            iter_count += 1\n",
    "            \n",
    "            # Logging\n",
    "            if iter_count % 100 == 0:\n",
    "                wandb.log({\n",
    "                    'stage2/iteration': iter_count,\n",
    "                    'stage2/loss_g': loss_g.item(),\n",
    "                    'stage2/loss_d': loss_d.item(),\n",
    "                    'stage2/l_pix': l_pix.item(),\n",
    "                    'stage2/l_vgg': l_vgg.item(),\n",
    "                    'stage2/l_adv': l_adv.item(),\n",
    "                })\n",
    "                print(f\"Iter {iter_count}: G={loss_g.item():.4f}, D={loss_d.item():.4f}, Pix={l_pix.item():.4f}\")\n",
    "            \n",
    "            # Save model every 5k iterations for ensemble - handle DataParallel wrapper\n",
    "            if iter_count % 5000 == 0:\n",
    "                model_path = f'generator_iter_{iter_count}.pth'\n",
    "                model_state = generator.module.state_dict() if isinstance(generator, nn.DataParallel) else generator.state_dict()\n",
    "                torch.save(model_state, model_path)\n",
    "                saved_models.append(model_path)\n",
    "                print(f\"✓ Saved checkpoint: {model_path}\")\n",
    "    \n",
    "    print(f\"\\n✓ Stage 2 complete. {len(saved_models)} checkpoints saved for ensemble.\")\n",
    "    return saved_models\n",
    "\n",
    "\n",
    "def ensemble_models(generator, model_paths, top_k=10):\n",
    "    \"\"\"Average parameters of top-k models\"\"\"\n",
    "    print(f\"\\nCreating ensemble from top-{top_k} models...\")\n",
    "    \n",
    "    # Select top-k models (simple: use last k models, or evaluate each)\n",
    "    selected_models = model_paths[-top_k:] if len(model_paths) >= top_k else model_paths\n",
    "    \n",
    "    # Average state dicts\n",
    "    ensemble_state = OrderedDict()\n",
    "    for path in selected_models:\n",
    "        state = torch.load(path, map_location=device)\n",
    "        for key in state:\n",
    "            if key not in ensemble_state:\n",
    "                ensemble_state[key] = state[key].clone()\n",
    "            else:\n",
    "                ensemble_state[key] += state[key]\n",
    "    \n",
    "    for key in ensemble_state:\n",
    "        ensemble_state[key] /= len(selected_models)\n",
    "    \n",
    "    # Load into generator - handle DataParallel wrapper\n",
    "    if isinstance(generator, nn.DataParallel):\n",
    "        generator.module.load_state_dict(ensemble_state)\n",
    "    else:\n",
    "        generator.load_state_dict(ensemble_state)\n",
    "    torch.save(ensemble_state, 'generator_ensemble.pth')\n",
    "    print(f\"✓ Ensemble model saved to generator_ensemble.pth\")\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "generator = Generator(\n",
    "    num_rrdb=wandb.config.num_rrdb,\n",
    "    num_rrfdb=wandb.config.num_rrfdb\n",
    ").to(device)\n",
    "\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# Wrap models with DataParallel to use multiple GPUs\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs with DataParallel!\")\n",
    "    generator = nn.DataParallel(generator)\n",
    "    discriminator = nn.DataParallel(discriminator)\n",
    "else:\n",
    "    print(\"Using single GPU\")\n",
    "\n",
    "print(f\"Generator params: {sum(p.numel() for p in generator.parameters())/1e6:.2f}M\")\n",
    "print(f\"Discriminator params: {sum(p.numel() for p in discriminator.parameters())/1e6:.2f}M\")\n",
    "\n",
    "# Execute training\n",
    "start_time = time.time()\n",
    "\n",
    "# Stage 1: PSNR training\n",
    "train_stage1(generator, train_loader, val_loader, \n",
    "             epochs=wandb.config.stage1_epochs, \n",
    "             lr=wandb.config.stage1_lr)\n",
    "\n",
    "# Stage 2: GAN training\n",
    "saved_models = train_stage2(generator, discriminator, train_loader, val_loader,\n",
    "                            iterations=wandb.config.stage2_iters,\n",
    "                            lr=wandb.config.stage2_lr)\n",
    "\n",
    "# Ensemble top models\n",
    "ensemble_models(generator, saved_models, top_k=wandb.config.ensemble_models)\n",
    "\n",
    "total_time = (time.time() - start_time) / 3600\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"✓ Training complete! Total time: {total_time:.2f} hours\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "wandb.log({'total_training_hours': total_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa60efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Inference and Evaluation\n",
    "\n",
    "def evaluate_model(generator, val_loader, num_samples=10):\n",
    "    \"\"\"Evaluate model and visualize results\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    total_psnr = 0\n",
    "    total_ssim = 0\n",
    "    sample_count = 0\n",
    "    \n",
    "    print(\"\\nEvaluating model...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (lr_img, hr_img) in enumerate(val_loader):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            lr_img = lr_img.to(device)\n",
    "            hr_img = hr_img.to(device)\n",
    "            \n",
    "            start = time.time()\n",
    "            sr_img = generator(lr_img)\n",
    "            inference_time = time.time() - start\n",
    "            \n",
    "            # PSNR\n",
    "            mse = F.mse_loss(sr_img, hr_img)\n",
    "            psnr = 10 * torch.log10(4 / mse)\n",
    "            total_psnr += psnr.item()\n",
    "            \n",
    "            # Simple SSIM approximation (for demo; use proper library for accuracy)\n",
    "            # ssim = ... (would need skimage or pytorch-msssim)\n",
    "            \n",
    "            sample_count += 1\n",
    "            \n",
    "            # Log samples to wandb\n",
    "            if i < 5:  # Log first 5 samples\n",
    "                lr_grid = (lr_img[0].cpu() + 1) / 2  # Denormalize\n",
    "                sr_grid = (sr_img[0].cpu() + 1) / 2\n",
    "                hr_grid = (hr_img[0].cpu() + 1) / 2\n",
    "                \n",
    "                wandb.log({\n",
    "                    f'samples/sample_{i}': [\n",
    "                        wandb.Image(lr_grid, caption='LR Input'),\n",
    "                        wandb.Image(sr_grid, caption='SR Output'),\n",
    "                        wandb.Image(hr_grid, caption='HR Ground Truth')\n",
    "                    ]\n",
    "                })\n",
    "    \n",
    "    avg_psnr = total_psnr / sample_count\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"  Average PSNR: {avg_psnr:.2f} dB\")\n",
    "    print(f\"  Inference time: {inference_time*1000:.2f} ms/image\")\n",
    "    \n",
    "    wandb.log({\n",
    "        'eval/psnr': avg_psnr,\n",
    "        'eval/inference_time_ms': inference_time * 1000\n",
    "    })\n",
    "    \n",
    "    return avg_psnr\n",
    "\n",
    "\n",
    "# Load best model and evaluate - handle DataParallel wrapper\n",
    "ensemble_state = torch.load('generator_ensemble.pth')\n",
    "if isinstance(generator, nn.DataParallel):\n",
    "    generator.module.load_state_dict(ensemble_state)\n",
    "else:\n",
    "    generator.load_state_dict(ensemble_state)\n",
    "\n",
    "final_psnr = evaluate_model(generator, val_loader, num_samples=20)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"FINAL RESULTS: PSNR = {final_psnr:.2f} dB\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n✓ All tasks complete! Check WandB dashboard for detailed metrics and visualizations.\")\n",
    "print(f\"   WandB Project: {wandb.run.project}\")\n",
    "print(f\"   Run URL: {wandb.run.url}\")\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
